{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 : Tennis \n",
    "\n",
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Env/Tennis.x86_64\",no_graphics=True)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents \n",
    "\n",
    "Let's define the models used for the actor and the critic of our two agents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state for one agent\n",
    "            action_size (int): Dimension of each action for one agent\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state (for all agents)\n",
    "            action_size (int): Dimension of each action (for all agents)\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to define the actors. The DDPG_Agent class corresponds to one of the multiple agents. The collaboration between them is handled by the next class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters of the training\n",
    "BUFFER_SIZE = int(1e6)   # replay buffer size\n",
    "BATCH_SIZE = 128         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 0.2               # for soft update of target parameters\n",
    "LR_ACTOR = 1*1e-4  # learning rate of the actor\n",
    "LR_CRITIC = 1*1e-3       # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "EPSILON_DECAY=1\n",
    "NB_UPDATES=5\n",
    "LEARN_EVERY=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class DDPG_Agent():\n",
    "    \"\"\" Agent that interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, nb_agents,random_seed,index=0,lr=LR_ACTOR,dec=EPSILON_DECAY):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            nb_agents (int) : number of agents in the environment\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.nb_agents=nb_agents\n",
    "        self.index=torch.cuda.LongTensor([index])\n",
    "        # Actor Network (w/ Target Network)\n",
    "        #### The actor only sees its own state\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)        \n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        #### The critic sees the whole state as well as the actions taken from the other agents\n",
    "        self.critic_local = Critic(state_size*nb_agents, action_size*nb_agents, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size*nb_agents, action_size*nb_agents, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "        self.epsilon = 1\n",
    "        self.decay = dec\n",
    "    \n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params:\n",
    "        =======\n",
    "            state : state seen by the current agent\n",
    "            add_noise (bool) : True if we add the UO noise\n",
    "        \"\"\"\n",
    "        if(not torch.is_tensor(state)):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action=(self.actor_local(state).cpu().data.numpy())\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            perturbation =self.noise.sample()*self.epsilon\n",
    "            action += perturbation\n",
    "        return np.clip(action, -1, 1)\n",
    "              \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, proposed_actions, proposed_next_actions,gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + Î³ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Collect the informations from experiences\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        next_actions = torch.cat(proposed_next_actions, dim=1).to(device)\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        with torch.no_grad():\n",
    "            Q_targets_next = self.critic_target(next_states, next_actions)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards.index_select(1,self.index) + (gamma * Q_targets_next * (1 - dones.index_select(1,self.index)))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        \n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets.detach())\n",
    "        \n",
    "        # Minimize the loss\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        ###### Guess an action\n",
    "        self.actor_optimizer.zero_grad()\n",
    "\n",
    "        actions_agent = [pa if i == self.index else pa.detach() for i, pa in enumerate(proposed_actions)]\n",
    "        actions_agent = torch.cat(actions_agent, dim=1).to(device)\n",
    "        actor_loss = -self.critic_local(states, actions_agent).mean()\n",
    "\n",
    "        # Minimize the loss\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ------------------------------ Reduce noise -------------------------- #\n",
    "        self.epsilon*=self.decay\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)     \n",
    "        \n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        Î¸_target = Ï*Î¸_local + (1 - Ï)*Î¸_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.actor_local.state_dict(), 'checkpoint_actor'+str(self.index.item())+'.pth')\n",
    "        torch.save(self.critic_local.state_dict(), 'checkpoint_critic'+str(self.index.item())+'.pth')\n",
    "    \n",
    "    def load(self):\n",
    "        self.critic_local.load_state_dict(torch.load('checkpoint_critic'+str(self.index.item())+'.pth'))\n",
    "        self.actor_local.load_state_dict(torch.load('checkpoint_actor'+str(self.index.item())+'.pth'))\n",
    "        self.critic_target.load_state_dict(torch.load('checkpoint_critic'+str(self.index.item())+'.pth'))\n",
    "        self.actor_target.load_state_dict(torch.load('checkpoint_actor'+str(self.index.item())+'.pth'))\n",
    "\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.size=size\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.normal(0,1,self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration\n",
    "\n",
    "The MultiAgent Deep Deterministic Policy Gradient (MADDPG) class handles the collaboration between agents. It initializes n agents, handles the buffer replay for them, and call their act and learn methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "\n",
    "class MADDPG():\n",
    "    \"\"\"  Deep Deterministic Policy Gradient (MADDPG) class handles the collaboration between agents. It initializes n agents, handles the buffer replay for them, and call their act and learn methods. \"\"\"\n",
    "    def __init__(self,state_size,action_size,nb_agents):\n",
    "        \"\"\"\n",
    "        @param state_size(int) : dimension of the state encountered by one agent\n",
    "        @param action_size(int): dimension of the actions taken by one agent\n",
    "        @param nb_agents(int) : number of agents in the environment\n",
    "        \"\"\"\n",
    "        random_seed=1\n",
    "        # Environment parameters\n",
    "        self.nb_agents=nb_agents\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        #Â Initialize the Agents. Each agent must know the dimension of its own state_size and action_size, as well as the number of other agents in the envorionment (as the critic depends on it) and its index in the agents list.\n",
    "        self.agents = [DDPG_Agent(state_size, action_size, nb_agents,random_seed,i) for i in range(0,nb_agents)]\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        self.steps=0\n",
    "        self.gamma=GAMMA\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state.reshape(1,-1), action, reward, next_state.reshape(1,-1), done)\n",
    "        self.steps+=1\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE and self.steps>LEARN_EVERY:\n",
    "            self.steps=0\n",
    "            for _ in range(0,NB_UPDATES):\n",
    "                # Each agent has to train : \n",
    "                # We start by sampling some experiences for each agents\n",
    "                experiences = [self.memory.sample() for _ in range(0,self.nb_agents)]\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def learn(self,experiences):\n",
    "        # Use actor target network to guess the optimal next action \n",
    "        # Use actor local network to compute the optimal action from the current sate\n",
    "        for i,agent_to_train in enumerate(self.agents):\n",
    "            states, _, _, next_states, _ = experiences[i]\n",
    "            next_actions=[]\n",
    "            actions=[]\n",
    "            for j,current_agent in enumerate(self.agents):\n",
    "                agent_id = torch.tensor([j]).to(device)\n",
    "                #Get the state and the next state seen by the current agent\n",
    "                experience_state = states.reshape(-1, self.nb_agents, self.state_size).index_select(1, agent_id).squeeze(1)\n",
    "                experience_next_state = next_states.reshape(-1, self.nb_agents, self.state_size).index_select(1, agent_id).squeeze(1)\n",
    "                # Get the action with target network and current network\n",
    "                agent_next_action = current_agent.actor_target(experience_next_state)\n",
    "                agent_action = current_agent.actor_local(experience_state)\n",
    "                actions.append(agent_action)\n",
    "                next_actions.append(agent_action)\n",
    "            agent_to_train.learn(experiences[i],actions,next_actions, self.gamma)\n",
    "        \n",
    "    def act(self,states):\n",
    "        actions = [current_agent.act(states[i]) for i,current_agent in enumerate(self.agents)]\n",
    "        return np.array(actions).reshape(1,-1)\n",
    "    \n",
    "    def reset(self):\n",
    "        for a in self.agents:\n",
    "            a.reset()\n",
    "            \n",
    "    def save(self):\n",
    "        for agent in self.agents:\n",
    "            agent.save()\n",
    "    def load(self):\n",
    "        for agent in self.agents:\n",
    "            agent.load()\n",
    "\n",
    "                    \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MADDPG(24,2,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(random_steps=0):\n",
    "    # Collect experience with pure random exploration\n",
    "    scores_agent = deque(maxlen=100)  # last 100 scores\n",
    "    print_every=25\n",
    "    my_scores=[]\n",
    "    for i in range(1,8001):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment   \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        agent.reset()\n",
    "        scores = np.zeros(2)                          # initialize the score (for each agent)\n",
    "        while True:            \n",
    "            actions=agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            agent.step(states, actions, rewards,next_states, dones)\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "            states = next_states  \n",
    "            \n",
    "        my_scores.append(np.max(scores))\n",
    "        scores_agent.append(np.max(scores))\n",
    "\n",
    "        if(np.mean(scores_agent)>0.5):\n",
    "            print('Environment Solved in {} episodes!'.format(i-100))\n",
    "            agent.save()\n",
    "            break\n",
    "        if (i%print_every==0):\n",
    "            print('Ep. {} : Av score (max over agents) over 100 last episodes {}'.format(i, np.mean(scores_agent)))\n",
    "            agent.save()\n",
    "    return my_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/.conda/envs/DRL/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 25 : Av score (max over agents) over 100 last episodes 0.004000000059604645\n",
      "Ep. 50 : Av score (max over agents) over 100 last episodes 0.007800000123679638\n",
      "Ep. 75 : Av score (max over agents) over 100 last episodes 0.005200000082453092\n",
      "Ep. 100 : Av score (max over agents) over 100 last episodes 0.00490000007674098\n",
      "Ep. 125 : Av score (max over agents) over 100 last episodes 0.003900000061839819\n",
      "Ep. 150 : Av score (max over agents) over 100 last episodes 0.0010000000149011613\n",
      "Ep. 175 : Av score (max over agents) over 100 last episodes 0.0010000000149011613\n",
      "Ep. 200 : Av score (max over agents) over 100 last episodes 0.0\n",
      "Ep. 225 : Av score (max over agents) over 100 last episodes 0.0\n",
      "Ep. 250 : Av score (max over agents) over 100 last episodes 0.0\n",
      "Ep. 275 : Av score (max over agents) over 100 last episodes 0.0010000000149011613\n",
      "Ep. 300 : Av score (max over agents) over 100 last episodes 0.0020000000298023225\n",
      "Ep. 325 : Av score (max over agents) over 100 last episodes 0.0029000000469386576\n",
      "Ep. 350 : Av score (max over agents) over 100 last episodes 0.008500000145286322\n",
      "Ep. 375 : Av score (max over agents) over 100 last episodes 0.010400000177323818\n",
      "Ep. 400 : Av score (max over agents) over 100 last episodes 0.013400000222027302\n",
      "Ep. 425 : Av score (max over agents) over 100 last episodes 0.019200000315904617\n",
      "Ep. 450 : Av score (max over agents) over 100 last episodes 0.01650000026449561\n",
      "Ep. 475 : Av score (max over agents) over 100 last episodes 0.02530000040307641\n",
      "Ep. 500 : Av score (max over agents) over 100 last episodes 0.029800000488758086\n",
      "Ep. 525 : Av score (max over agents) over 100 last episodes 0.034500000569969415\n",
      "Ep. 550 : Av score (max over agents) over 100 last episodes 0.045900000762194394\n",
      "Ep. 575 : Av score (max over agents) over 100 last episodes 0.054700000900775196\n",
      "Ep. 600 : Av score (max over agents) over 100 last episodes 0.06710000110790133\n",
      "Ep. 625 : Av score (max over agents) over 100 last episodes 0.07210000118240714\n",
      "Ep. 650 : Av score (max over agents) over 100 last episodes 0.0754000012204051\n",
      "Ep. 675 : Av score (max over agents) over 100 last episodes 0.07440000120550394\n",
      "Ep. 700 : Av score (max over agents) over 100 last episodes 0.07110000113025308\n",
      "Ep. 725 : Av score (max over agents) over 100 last episodes 0.0731000011600554\n",
      "Ep. 750 : Av score (max over agents) over 100 last episodes 0.07840000126510858\n",
      "Ep. 775 : Av score (max over agents) over 100 last episodes 0.07600000124424695\n",
      "Ep. 800 : Av score (max over agents) over 100 last episodes 0.0773000012524426\n",
      "Ep. 825 : Av score (max over agents) over 100 last episodes 0.07750000124797225\n",
      "Ep. 850 : Av score (max over agents) over 100 last episodes 0.07870000125840307\n",
      "Ep. 875 : Av score (max over agents) over 100 last episodes 0.08030000125989317\n",
      "Ep. 900 : Av score (max over agents) over 100 last episodes 0.08290000131353736\n",
      "Ep. 925 : Av score (max over agents) over 100 last episodes 0.08120000127702952\n",
      "Ep. 950 : Av score (max over agents) over 100 last episodes 0.07020000111311674\n",
      "Ep. 975 : Av score (max over agents) over 100 last episodes 0.07220000114291907\n",
      "Ep. 1000 : Av score (max over agents) over 100 last episodes 0.07450000116601586\n",
      "Ep. 1025 : Av score (max over agents) over 100 last episodes 0.07840000122785568\n",
      "Ep. 1050 : Av score (max over agents) over 100 last episodes 0.0903000013716519\n",
      "Ep. 1075 : Av score (max over agents) over 100 last episodes 0.09460000142455101\n",
      "Ep. 1100 : Av score (max over agents) over 100 last episodes 0.09750000147148967\n",
      "Ep. 1125 : Av score (max over agents) over 100 last episodes 0.10050000151619315\n",
      "Ep. 1150 : Av score (max over agents) over 100 last episodes 0.10200000155717134\n",
      "Ep. 1175 : Av score (max over agents) over 100 last episodes 0.10590000161901117\n",
      "Ep. 1200 : Av score (max over agents) over 100 last episodes 0.1129000017233193\n",
      "Ep. 1225 : Av score (max over agents) over 100 last episodes 0.1240000018849969\n",
      "Ep. 1250 : Av score (max over agents) over 100 last episodes 0.13640000205487013\n",
      "Ep. 1275 : Av score (max over agents) over 100 last episodes 0.13340000201016664\n",
      "Ep. 1300 : Av score (max over agents) over 100 last episodes 0.14370000215247272\n",
      "Ep. 1325 : Av score (max over agents) over 100 last episodes 0.14370000215247272\n",
      "Ep. 1350 : Av score (max over agents) over 100 last episodes 0.14480000216513872\n",
      "Ep. 1375 : Av score (max over agents) over 100 last episodes 0.15490000231191517\n",
      "Ep. 1400 : Av score (max over agents) over 100 last episodes 0.152800002284348\n",
      "Ep. 1425 : Av score (max over agents) over 100 last episodes 0.16990000253543258\n",
      "Ep. 1450 : Av score (max over agents) over 100 last episodes 0.17260000260546804\n",
      "Ep. 1475 : Av score (max over agents) over 100 last episodes 0.18740000283345581\n",
      "Ep. 1500 : Av score (max over agents) over 100 last episodes 0.21850000329315664\n",
      "Ep. 1525 : Av score (max over agents) over 100 last episodes 0.22730000343173742\n",
      "Ep. 1550 : Av score (max over agents) over 100 last episodes 0.249600003734231\n",
      "Ep. 1575 : Av score (max over agents) over 100 last episodes 0.25340000379830596\n",
      "Ep. 1600 : Av score (max over agents) over 100 last episodes 0.23680000357329845\n",
      "Ep. 1625 : Av score (max over agents) over 100 last episodes 0.23390000352635978\n",
      "Ep. 1650 : Av score (max over agents) over 100 last episodes 0.20860000316053628\n",
      "Ep. 1675 : Av score (max over agents) over 100 last episodes 0.21500000324100255\n",
      "Ep. 1700 : Av score (max over agents) over 100 last episodes 0.2075000031106174\n",
      "Ep. 1725 : Av score (max over agents) over 100 last episodes 0.18020000271499156\n",
      "Ep. 1750 : Av score (max over agents) over 100 last episodes 0.1810000027343631\n",
      "Ep. 1775 : Av score (max over agents) over 100 last episodes 0.2138000032491982\n",
      "Ep. 1800 : Av score (max over agents) over 100 last episodes 0.25380000384524465\n",
      "Ep. 1825 : Av score (max over agents) over 100 last episodes 0.3306000049971044\n",
      "Ep. 1850 : Av score (max over agents) over 100 last episodes 0.3910000058822334\n",
      "Ep. 1875 : Av score (max over agents) over 100 last episodes 0.42120000630617144\n",
      "Ep. 1900 : Av score (max over agents) over 100 last episodes 0.4081000061146915\n",
      "Ep. 1925 : Av score (max over agents) over 100 last episodes 0.38430000577121975\n",
      "Ep. 1950 : Av score (max over agents) over 100 last episodes 0.35030000526458027\n",
      "Ep. 1975 : Av score (max over agents) over 100 last episodes 0.2930000044219196\n",
      "Ep. 2000 : Av score (max over agents) over 100 last episodes 0.28710000433027744\n",
      "Ep. 2025 : Av score (max over agents) over 100 last episodes 0.2994000044837594\n",
      "Ep. 2050 : Av score (max over agents) over 100 last episodes 0.3031000045500696\n",
      "Ep. 2075 : Av score (max over agents) over 100 last episodes 0.33800000507384537\n",
      "Ep. 2100 : Av score (max over agents) over 100 last episodes 0.33550000505521893\n",
      "Ep. 2125 : Av score (max over agents) over 100 last episodes 0.33120000500231983\n",
      "Ep. 2150 : Av score (max over agents) over 100 last episodes 0.37230000561103227\n",
      "Ep. 2175 : Av score (max over agents) over 100 last episodes 0.353400005325675\n",
      "Ep. 2200 : Av score (max over agents) over 100 last episodes 0.3944000059366226\n",
      "Ep. 2225 : Av score (max over agents) over 100 last episodes 0.39860000599175693\n",
      "Ep. 2250 : Av score (max over agents) over 100 last episodes 0.40460000608116387\n",
      "Ep. 2275 : Av score (max over agents) over 100 last episodes 0.40020000603049993\n",
      "Ep. 2300 : Av score (max over agents) over 100 last episodes 0.3726000056043267\n",
      "Ep. 2325 : Av score (max over agents) over 100 last episodes 0.33820000510662795\n",
      "Ep. 2350 : Av score (max over agents) over 100 last episodes 0.3231000048853457\n",
      "Ep. 2375 : Av score (max over agents) over 100 last episodes 0.3574000054039061\n",
      "Ep. 2400 : Av score (max over agents) over 100 last episodes 0.3933000059425831\n",
      "Ep. 2425 : Av score (max over agents) over 100 last episodes 0.4138000062294304\n",
      "Ep. 2450 : Av score (max over agents) over 100 last episodes 0.4269000064395368\n",
      "Ep. 2475 : Av score (max over agents) over 100 last episodes 0.4240000063739717\n",
      "Ep. 2500 : Av score (max over agents) over 100 last episodes 0.3826000057905912\n",
      "Ep. 2525 : Av score (max over agents) over 100 last episodes 0.40740000616759064\n",
      "Ep. 2550 : Av score (max over agents) over 100 last episodes 0.40300000611692666\n",
      "Ep. 2575 : Av score (max over agents) over 100 last episodes 0.39260000597685574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 2600 : Av score (max over agents) over 100 last episodes 0.39400000596418977\n",
      "Ep. 2625 : Av score (max over agents) over 100 last episodes 0.3489000052958727\n",
      "Ep. 2650 : Av score (max over agents) over 100 last episodes 0.3453000052087009\n",
      "Ep. 2675 : Av score (max over agents) over 100 last episodes 0.33080000497400763\n",
      "Ep. 2700 : Av score (max over agents) over 100 last episodes 0.3439000051654875\n",
      "Ep. 2725 : Av score (max over agents) over 100 last episodes 0.40070000605657696\n",
      "Ep. 2750 : Av score (max over agents) over 100 last episodes 0.3805000057630241\n",
      "Ep. 2775 : Av score (max over agents) over 100 last episodes 0.368600005581975\n",
      "Ep. 2800 : Av score (max over agents) over 100 last episodes 0.38820000590756537\n",
      "Ep. 2825 : Av score (max over agents) over 100 last episodes 0.34950000528246167\n",
      "Ep. 2850 : Av score (max over agents) over 100 last episodes 0.3267000049352646\n",
      "Ep. 2875 : Av score (max over agents) over 100 last episodes 0.31350000474601986\n",
      "Ep. 2900 : Av score (max over agents) over 100 last episodes 0.28280000425875185\n",
      "Ep. 2925 : Av score (max over agents) over 100 last episodes 0.31760000478476286\n",
      "Ep. 2950 : Av score (max over agents) over 100 last episodes 0.3697000055573881\n",
      "Ep. 2975 : Av score (max over agents) over 100 last episodes 0.4158000062406063\n",
      "Ep. 3000 : Av score (max over agents) over 100 last episodes 0.4445000066794455\n",
      "Ep. 3025 : Av score (max over agents) over 100 last episodes 0.42970000645145773\n",
      "Ep. 3050 : Av score (max over agents) over 100 last episodes 0.4274000064469874\n",
      "Ep. 3075 : Av score (max over agents) over 100 last episodes 0.4354000065661967\n",
      "Ep. 3100 : Av score (max over agents) over 100 last episodes 0.40460000609979035\n",
      "Ep. 3125 : Av score (max over agents) over 100 last episodes 0.42640000643208625\n",
      "Ep. 3150 : Av score (max over agents) over 100 last episodes 0.4224000063538551\n",
      "Ep. 3175 : Av score (max over agents) over 100 last episodes 0.4732000071182847\n",
      "Environment Solved in 3092 episodes!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "scores = train()\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Watch the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/.conda/envs/DRL/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 1 : Av score (max over agents) over 100 last episodes 0.09000000171363354\n",
      "Ep. 2 : Av score (max over agents) over 100 last episodes 0.09000000171363354\n",
      "Ep. 3 : Av score (max over agents) over 100 last episodes 0.060000001142422356\n",
      "Ep. 4 : Av score (max over agents) over 100 last episodes 0.04500000085681677\n",
      "Ep. 5 : Av score (max over agents) over 100 last episodes 0.036000000685453414\n",
      "Ep. 6 : Av score (max over agents) over 100 last episodes 0.04500000085681677\n",
      "Ep. 7 : Av score (max over agents) over 100 last episodes 0.038571429305842946\n",
      "Ep. 8 : Av score (max over agents) over 100 last episodes 0.033750000642612576\n",
      "Ep. 9 : Av score (max over agents) over 100 last episodes 0.030000000571211178\n",
      "Ep. 10 : Av score (max over agents) over 100 last episodes 0.027000000514090062\n",
      "Ep. 11 : Av score (max over agents) over 100 last episodes 0.024545455012809147\n",
      "Ep. 12 : Av score (max over agents) over 100 last episodes 0.030000000571211178\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-40bab4677373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m        \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m            \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m            \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m           \u001b[0;31m# send all actions to the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m            \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m         \u001b[0;31m# get next state (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m            \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m                         \u001b[0;31m# get reward (for each agent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DRL/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             outputs = self.communicator.exchange(\n\u001b[0;32m--> 369\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             )\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DRL/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DRL/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DRL/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DRL/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Load the agent\n",
    "agent.load()\n",
    "\n",
    "# Watch it \n",
    "scores_agent= []\n",
    "my_scores=[]\n",
    "print_every=1\n",
    "for i in range(1,101):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]     # reset the environment   \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        agent.reset()\n",
    "        scores = np.zeros(2)                          # initialize the score (for each agent)\n",
    "        while True:            \n",
    "            actions=agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "            states = next_states  \n",
    "            \n",
    "        my_scores.append(np.max(scores))\n",
    "        scores_agent.append(np.max(scores))\n",
    "\n",
    "        if (i%print_every==0):\n",
    "            print('Ep. {} : Av score (max over agents) over 100 last episodes {}'.format(i, np.mean(scores_agent)))\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
